xg.wide <- xg
xg <- xg.deep
https://www.dropbox.com/s/u711eela48rx4og/Skjermbilde%202018-04-24%2019.04.06.png?dl=0
### NEW prediction test
fixtures <- data.table(read.csv('data/future/fixtures.csv'))
fixtures <- fixtures[fixtures$Div %in% dt.pr$Div]
fixtures <- fixtures[,c('Div','Date','HomeTeam','AwayTeam'),with=FALSE]
fixtures <- convert.and.sort.date(fixtures)
fixtures <- trim.whitespace.factors(fixtures)
dt.predict <- data.table(matrix(0,nrow = nrow(fixtures),ncol = length(all.names)))
colnames(dt.predict) = all.names
for (row in 1:nrow(fixtures)) {
date <- fixtures[row]$Date
hometeam <- fixtures[row]$HomeTeam
awayteam <- fixtures[row]$AwayTeam
for (i in 1:(length(data.columns))) {
#Slow maybe because of warnings, idk why tho
stat <- data.columns[i]
stat.names.hf <- paste(stat,'hf.names',sep='')
stat.names.ha <- paste(stat,'ha.names',sep='')
stat.names.af <- paste(stat,'af.names',sep='')
stat.names.aa <- paste(stat,'aa.names',sep='')
feature.hf <- eval(parse(text=stat.names.hf))   #Home-team, for
feature.ha <- eval(parse(text=stat.names.ha))  #Home-team, against
feature.af <- eval(parse(text=stat.names.af)) #Away-team, for
feature.aa <- eval(parse(text=stat.names.aa))   #Away-team, against
dt.predict[row,(feature.hf) := as.list(rev(exp.discount(get.home.stat(dt, stat, hometeam,
date, k))))]
dt.predict[row,(feature.ha) := as.list(rev(exp.discount(get.away.stat(dt, stat, hometeam,
date, k))))]
dt.predict[row,(feature.af) := as.list(rev(exp.discount(get.home.stat(dt, stat, awayteam,
date, k))))]
dt.predict[row,(feature.aa) := as.list(rev(exp.discount(get.away.stat(dt, stat, awayteam,
date, k))))]
}
}
predictions <- predict(xg,xgb.DMatrix(data = as.matrix(dt.predict[,-c('Y','Div')])),xg$best_iteration)
fixtures[,'predictions' := predictions]
fixtures
get.accuracy2(pred.vals,test$Y,0.3259290)
get.accuracy2(pred.vals,test$Y,0.3259290,incr = 0.025)
get.accuracy2(pred.vals,test$Y,0.3259290,incr = 0.1)
get.accuracy2(pred.vals,test$Y,0.3259290,incr = 0.05)
get.accuracy <- function(pred.vals, Y, p) {
# Get expected accuracy of model output p given pred.vals
# Note: this works as an (empricial) odds converter
cutoff = 0.5
if (p < cutoff) {
acc = sum(pred.vals[Y == 0] < p)/sum(pred.vals < p)
} else {
acc = sum(pred.vals[Y == 1] > p)/sum(pred.vals > p)
}
return(acc)
}
pred.vals <- predict(xg,dtest)#,ntreelimit = 996)
acc = c()
acc2 = c()
p.vec = seq(0.01,1,by=0.001)
for (i in 1:length(p.vec)) {
p = p.vec[i]
acc[i] = get.accuracy(pred.vals, test$Y, p)
i = i +1
}
plot(p.vec,acc,'l',xlim = c(0.25,0.75),ylim=c(0.35, 1))
### ORDINARY TRAIN
tune.grid <- expand.grid(eta = c(0.002),#c(0.001587127),
gamma = c(0.4366591 ),
max_depth = c(8),#c(10),
min_child_weight = c(2.117832),
subsample = c(0.7052139),
colsample_bytree = c(0.9965859),
lambda = c(1.085726),
alpha = c(0.0001228216))
xg <- xgb.train(params = tune.grid,
data = dtrain,
nrounds = 4000,
objective = "binary:logistic",
#eval_metric = 'auc',
#maximize = TRUE,
eval_metric = 'error@0.5',
early_stopping_rounds = 40,
watchlist = watchlist,
print_every_n = 1)
tune.grid
get.accuracy <- function(pred.vals, Y, p) {
# Get expected accuracy of model output p given pred.vals
# Note: this works as an (empricial) odds converter
cutoff = 0.5
if (p < cutoff) {
acc = sum(pred.vals[Y == 0] < p)/sum(pred.vals < p)
} else {
acc = sum(pred.vals[Y == 1] > p)/sum(pred.vals > p)
}
return(acc)
}
pred.vals <- predict(xg,dtest)#,ntreelimit = 996)
acc = c()
acc2 = c()
p.vec = seq(0.01,1,by=0.001)
get.accuracy <- function(pred.vals, Y, p) {
# Get expected accuracy of model output p given pred.vals
# Note: this works as an (empricial) odds converter
cutoff = 0.5
if (p < cutoff) {
acc = sum(pred.vals[Y == 0] < p)/sum(pred.vals < p)
} else {
acc = sum(pred.vals[Y == 1] > p)/sum(pred.vals > p)
}
return(acc)
}
pred.vals <- predict(xg,dtest)#,ntreelimit = 996)
acc = c()
acc2 = c()
p.vec = seq(0.01,1,by=0.001)
for (i in 1:length(p.vec)) {
p = p.vec[i]
acc[i] = get.accuracy(pred.vals, test$Y, p)
i = i +1
}
plot(p.vec,acc,'l',xlim = c(0.25,0.75),ylim=c(0.35, 1))
### NEW prediction test
fixtures <- data.table(read.csv('data/future/fixtures.csv'))
fixtures <- fixtures[fixtures$Div %in% dt.pr$Div]
fixtures <- fixtures[,c('Div','Date','HomeTeam','AwayTeam'),with=FALSE]
fixtures <- convert.and.sort.date(fixtures)
fixtures <- trim.whitespace.factors(fixtures)
dt.predict <- data.table(matrix(0,nrow = nrow(fixtures),ncol = length(all.names)))
colnames(dt.predict) = all.names
for (row in 1:nrow(fixtures)) {
date <- fixtures[row]$Date
hometeam <- fixtures[row]$HomeTeam
awayteam <- fixtures[row]$AwayTeam
for (i in 1:(length(data.columns))) {
#Slow maybe because of warnings, idk why tho
stat <- data.columns[i]
stat.names.hf <- paste(stat,'hf.names',sep='')
stat.names.ha <- paste(stat,'ha.names',sep='')
stat.names.af <- paste(stat,'af.names',sep='')
stat.names.aa <- paste(stat,'aa.names',sep='')
feature.hf <- eval(parse(text=stat.names.hf))   #Home-team, for
feature.ha <- eval(parse(text=stat.names.ha))  #Home-team, against
feature.af <- eval(parse(text=stat.names.af)) #Away-team, for
feature.aa <- eval(parse(text=stat.names.aa))   #Away-team, against
dt.predict[row,(feature.hf) := as.list(rev(exp.discount(get.home.stat(dt, stat, hometeam,
date, k))))]
dt.predict[row,(feature.ha) := as.list(rev(exp.discount(get.away.stat(dt, stat, hometeam,
date, k))))]
dt.predict[row,(feature.af) := as.list(rev(exp.discount(get.home.stat(dt, stat, awayteam,
date, k))))]
dt.predict[row,(feature.aa) := as.list(rev(exp.discount(get.away.stat(dt, stat, awayteam,
date, k))))]
}
}
predictions <- predict(xg,xgb.DMatrix(data = as.matrix(dt.predict[,-c('Y','Div')])),xg$best_iteration)
fixtures[,'predictions' := predictions]
fixtures
xg.deep$params
xg.wide$params
xg.wide$best_score
min(xg.wide$evaluation_log$test_auc)
min(xg.deep$evaluation_log$test_auc)
xg.deep$evaluation_log
xg.deep$evaluation_log$test_error
min(xg.deep$evaluation_log$test_error)
### ORDINARY TRAIN
tune.grid <- expand.grid(eta = c(0.002),#c(0.001587127),
gamma = c(0.4366591 ),
max_depth = c(10),#c(10),
min_child_weight = c(2.117832),
subsample = c(0.7052139),
colsample_bytree = c(0.9965859),
lambda = c(1.085726),
alpha = c(0.0001228216))
require(xgboost)
require(data.table)
dt.pr <- data.table(read.csv('data/processed/data_5_corners_meanvar.csv'))
mean.var <- read.csv('data/processed/meanvar_5_corners_meanvar.csv',row.names = 1,header = TRUE)
dt.pr[,X := NULL]
#dt.pr <- data.table(read.csv('data/processed/data_5_corners.csv'))
#dt.pr[,Y := as.integer(dt.pr[,Y > class.limit])]
create.one.hot <- function(dt.pr) {
one.hot.mat <- as.matrix(model.matrix(~Div+0,dt.pr))
dt.pr <- cbind(dt.pr,as.matrix(model.matrix(~Div+0,dt.pr)))
return(dt.pr)
}
recover.identity <- function(dt.pr) {
# Recover identity for corner stat
home.corners <- (dt.pr$Y_1 * mean.var['HC','var']) + mean.var['HC','mean']
away.corners <- (dt.pr$Y_2 * mean.var['AC','var']) + mean.var['AC','mean']
dt.pr[,'corners' := (home.corners + away.corners)]
}
dt.pr <- recover.identity(dt.pr)
dt.pr <- create.one.hot(dt.pr)
class.limit <- 9
dt.pr[,Y := as.integer(dt.pr[,corners > class.limit])]
# Adjust sample size (check with other computer for best function)
#sample.idx <- sample(nrow(dt.pr[dt.pr[,Y2 == 0],]),nrow(dt.pr[dt.pr[,Y2==1]]))
#dt.true <- dt.pr[sample.idx,]
#dt.pr <- rbind(dt.true,dt.pr[dt.pr[,Y2 == 1]])
p = 0.8 #Train/test proportion
smp_size <- floor(p * nrow(dt.pr))
## set the seed to make your partition reproductible
set.seed(123)
train_ind <- sample(seq_len(nrow(dt.pr)), size = smp_size)
train <- dt.pr[train_ind, ]
test <- dt.pr[-train_ind, ]
# New (meanvar) data
dtrain <- xgb.DMatrix(data = as.matrix(train[,-c('Y','Y_1','Y_2','corners','Div')]), label= as.matrix(train[,Y]))
dtest <- xgb.DMatrix(data = as.matrix(test[,-c('Y','Y_1','Y_2','corners','Div')]), label=as.matrix(test[,Y]))
# Old data
#dtrain <- xgb.DMatrix(data = as.matrix(train[,-c('Y','Div','div')]), label= as.matrix(train[,Y]))
#dtrain <- xgb.DMatrix(data = as.matrix(test[,-c('Y','Div','div')]), label= as.matrix(test[,Y]))
watchlist <- list(test=dtest,train=dtrain)
### ORDINARY TRAIN
tune.grid <- expand.grid(eta = c(0.002),#c(0.001587127),
gamma = c(0.4366591 ),
max_depth = c(10),#c(10),
min_child_weight = c(2.117832),
subsample = c(0.7052139),
colsample_bytree = c(0.9965859),
lambda = c(1.085726),
alpha = c(0.0001228216))
xg <- xgb.train(params = tune.grid,
data = dtrain,
nrounds = 4000,
objective = "binary:logistic",
eval_metric = 'auc',
maximize = TRUE,
#eval_metric = 'error@0.5',
early_stopping_rounds = 40,
watchlist = watchlist,
print_every_n = 1)
dt.predict
get.accuracy <- function(pred.vals, Y, p) {
# Get expected accuracy of model output p given pred.vals
# Note: this works as an (empricial) odds converter
cutoff = 0.5
if (p < cutoff) {
acc = sum(pred.vals[Y == 0] < p)/sum(pred.vals < p)
} else {
acc = sum(pred.vals[Y == 1] > p)/sum(pred.vals > p)
}
return(acc)
}
get.accuracy2 <- function(pred.vals, Y, model.prob,incr = 0.05) {
i = 1
acc.vec = c()
p.vec = seq(0,(1-incr),by=incr)
for (p in p.vec) {
if (p < 0.5) {
acc = sum(p < pred.vals[Y==0] & pred.vals[Y==0] < p+incr)/ sum(p < pred.vals & pred.vals < p+incr)
}
else{
acc = sum(p < pred.vals[Y==1] & pred.vals[Y==1] < p+incr)/ sum(p < pred.vals & pred.vals < p+incr)
}
if (acc == 'NaN'){
acc.vec[i] = 0
} else{
acc.vec[i] = acc
}
i = i+1
}
pos = sum(model.prob > p.vec)
mean.acc = (acc.vec[pos] + acc.vec[pos+1])/2
return(mean.acc)
}
get.accuracy <- function(pred.vals, Y, p) {
# Get expected accuracy of model output p given pred.vals
# Note: this works as an (empricial) odds converter
cutoff = 0.5
if (p < cutoff) {
acc = sum(pred.vals[Y == 0] < p)/sum(pred.vals < p)
} else {
acc = sum(pred.vals[Y == 1] > p)/sum(pred.vals > p)
}
return(acc)
}
pred.vals <- predict(xg,dtest)#,ntreelimit = 996)
acc = c()
acc2 = c()
p.vec = seq(0.01,1,by=0.001)
for (i in 1:length(p.vec)) {
p = p.vec[i]
acc[i] = get.accuracy(pred.vals, test$Y, p)
acc2[i] = get.accuract(pred.vals, test$Y, p)
i = i +1
}
plot(p.vec,acc,'l',xlim = c(0.25,0.75),ylim=c(0.35, 1))
lines(p.vec,acc2,'l',xlim = c(0.25,0.75),ylim=c(0.35, 1))
length(acc2)
for (i in 1:length(p.vec)) {
p = p.vec[i]
acc[i] = get.accuracy(pred.vals, test$Y, p)
acc2[i] = get.accuracy2(pred.vals, test$Y, p)
i = i +1
}
plot(p.vec,acc,'l',xlim = c(0.25,0.75),ylim=c(0.35, 1))
lines(p.vec,acc2,'l',xlim = c(0.25,0.75),ylim=c(0.35, 1))
plot(p.vec,acc,'l',xlim = c(0.25,0.75),ylim=c(0.35, 1),col='red')
lines(p.vec,acc2,'l',xlim = c(0.25,0.75),ylim=c(0.35, 1),col='green')
for (i in 1:length(p.vec)) {
p = p.vec[i]
acc[i] = get.accuracy(pred.vals, test$Y, p)
acc2[i] = get.accuracy2(pred.vals, test$Y, p,incr=0.025)
i = i +1
}
plot(p.vec,acc,'l',xlim = c(0.25,0.75),ylim=c(0.35, 1),col='red')
lines(p.vec,acc2,'l',xlim = c(0.25,0.75),ylim=c(0.35, 1),col='green')
### NEW prediction test
fixtures <- data.table(read.csv('data/future/fixtures.csv'))
fixtures <- fixtures[fixtures$Div %in% dt.pr$Div]
fixtures <- fixtures[,c('Div','Date','HomeTeam','AwayTeam'),with=FALSE]
fixtures <- convert.and.sort.date(fixtures)
fixtures <- trim.whitespace.factors(fixtures)
dt.predict <- data.table(matrix(0,nrow = nrow(fixtures),ncol = length(all.names)))
colnames(dt.predict) = all.names
for (row in 1:nrow(fixtures)) {
date <- fixtures[row]$Date
hometeam <- fixtures[row]$HomeTeam
awayteam <- fixtures[row]$AwayTeam
for (i in 1:(length(data.columns))) {
#Slow maybe because of warnings, idk why tho
stat <- data.columns[i]
stat.names.hf <- paste(stat,'hf.names',sep='')
stat.names.ha <- paste(stat,'ha.names',sep='')
stat.names.af <- paste(stat,'af.names',sep='')
stat.names.aa <- paste(stat,'aa.names',sep='')
feature.hf <- eval(parse(text=stat.names.hf))   #Home-team, for
feature.ha <- eval(parse(text=stat.names.ha))  #Home-team, against
feature.af <- eval(parse(text=stat.names.af)) #Away-team, for
feature.aa <- eval(parse(text=stat.names.aa))   #Away-team, against
dt.predict[row,(feature.hf) := as.list(rev(exp.discount(get.home.stat(dt, stat, hometeam,
date, k))))]
dt.predict[row,(feature.ha) := as.list(rev(exp.discount(get.away.stat(dt, stat, hometeam,
date, k))))]
dt.predict[row,(feature.af) := as.list(rev(exp.discount(get.home.stat(dt, stat, awayteam,
date, k))))]
dt.predict[row,(feature.aa) := as.list(rev(exp.discount(get.away.stat(dt, stat, awayteam,
date, k))))]
}
}
predictions <- predict(xg,xgb.DMatrix(data = as.matrix(dt.predict[,-c('Y','Div')])),xg$best_iteration)
fixtures[,'predictions' := predictions]
fixtures
xg.9 <- xg
tune.grid <- expand.grid(eta = c(0.01),
gamma = c(0.5),
max_depth = c(10),
min_child_weight = c(3.0),
subsample = c(0.7),
colsample_bytree = c(0.6),
lambda = c(1.5),
alpha = c(0.0001))
xg <- xgb.train(params = tune.grid,
data = dtrain,
nrounds = 4000,
objective = "binary:logistic",
#eval_metric = 'auc',
#maximize = TRUE,
eval_metric = 'error@0.5',
early_stopping_rounds = 40,
watchlist = watchlist,
print_every_n = 1)
get.accuracy2 <- function(pred.vals, Y, model.prob,incr = 0.05) {
i = 1
acc.vec = c()
p.vec = seq(0,(1-incr),by=incr)
for (p in p.vec) {
if (p < 0.5) {
acc = sum(p < pred.vals[Y==0] & pred.vals[Y==0] < p+incr)/ sum(p < pred.vals & pred.vals < p+incr)
}
else{
acc = sum(p < pred.vals[Y==1] & pred.vals[Y==1] < p+incr)/ sum(p < pred.vals & pred.vals < p+incr)
}
if (acc == 'NaN'){
acc.vec[i] = 0
} else{
acc.vec[i] = acc
}
i = i+1
}
pos = sum(model.prob > p.vec)
mean.acc = (acc.vec[pos] + acc.vec[pos+1])/2
return(mean.acc)
}
get.accuracy <- function(pred.vals, Y, p) {
# Get expected accuracy of model output p given pred.vals
# Note: this works as an (empricial) odds converter
cutoff = 0.5
if (p < cutoff) {
acc = sum(pred.vals[Y == 0] < p)/sum(pred.vals < p)
} else {
acc = sum(pred.vals[Y == 1] > p)/sum(pred.vals > p)
}
return(acc)
}
pred.vals <- predict(xg,dtest)#,ntreelimit = 996)
acc = c()
acc2 = c()
p.vec = seq(0.01,1,by=0.001)
for (i in 1:length(p.vec)) {
p = p.vec[i]
acc[i] = get.accuracy(pred.vals, test$Y, p)
acc2[i] = get.accuracy2(pred.vals, test$Y, p,incr=0.025)
i = i +1
}
plot(p.vec,acc,'l',xlim = c(0.25,0.75),ylim=c(0.35, 1),col='red')
lines(p.vec,acc2,'l',xlim = c(0.25,0.75),ylim=c(0.35, 1),col='green')
### NEW prediction test
fixtures <- data.table(read.csv('data/future/fixtures.csv'))
fixtures <- fixtures[fixtures$Div %in% dt.pr$Div]
fixtures <- fixtures[,c('Div','Date','HomeTeam','AwayTeam'),with=FALSE]
fixtures <- convert.and.sort.date(fixtures)
fixtures <- trim.whitespace.factors(fixtures)
dt.predict <- data.table(matrix(0,nrow = nrow(fixtures),ncol = length(all.names)))
colnames(dt.predict) = all.names
for (row in 1:nrow(fixtures)) {
date <- fixtures[row]$Date
hometeam <- fixtures[row]$HomeTeam
awayteam <- fixtures[row]$AwayTeam
for (i in 1:(length(data.columns))) {
#Slow maybe because of warnings, idk why tho
stat <- data.columns[i]
stat.names.hf <- paste(stat,'hf.names',sep='')
stat.names.ha <- paste(stat,'ha.names',sep='')
stat.names.af <- paste(stat,'af.names',sep='')
stat.names.aa <- paste(stat,'aa.names',sep='')
feature.hf <- eval(parse(text=stat.names.hf))   #Home-team, for
feature.ha <- eval(parse(text=stat.names.ha))  #Home-team, against
feature.af <- eval(parse(text=stat.names.af)) #Away-team, for
feature.aa <- eval(parse(text=stat.names.aa))   #Away-team, against
dt.predict[row,(feature.hf) := as.list(rev(exp.discount(get.home.stat(dt, stat, hometeam,
date, k))))]
dt.predict[row,(feature.ha) := as.list(rev(exp.discount(get.away.stat(dt, stat, hometeam,
date, k))))]
dt.predict[row,(feature.af) := as.list(rev(exp.discount(get.home.stat(dt, stat, awayteam,
date, k))))]
dt.predict[row,(feature.aa) := as.list(rev(exp.discount(get.away.stat(dt, stat, awayteam,
date, k))))]
}
}
predictions <- predict(xg,xgb.DMatrix(data = as.matrix(dt.predict[,-c('Y','Div')])),xg$best_iteration)
fixtures[,'predictions' := predictions]
fixtures
get.accuracy2(pred.vals,test$Y,0.46)
1/0.5423387
xg.9.auc <- xg
xg <- xg.9
### NEW prediction test
fixtures <- data.table(read.csv('data/future/fixtures.csv'))
fixtures <- fixtures[fixtures$Div %in% dt.pr$Div]
fixtures <- fixtures[,c('Div','Date','HomeTeam','AwayTeam'),with=FALSE]
fixtures <- convert.and.sort.date(fixtures)
fixtures <- trim.whitespace.factors(fixtures)
dt.predict <- data.table(matrix(0,nrow = nrow(fixtures),ncol = length(all.names)))
colnames(dt.predict) = all.names
for (row in 1:nrow(fixtures)) {
date <- fixtures[row]$Date
hometeam <- fixtures[row]$HomeTeam
awayteam <- fixtures[row]$AwayTeam
for (i in 1:(length(data.columns))) {
#Slow maybe because of warnings, idk why tho
stat <- data.columns[i]
stat.names.hf <- paste(stat,'hf.names',sep='')
stat.names.ha <- paste(stat,'ha.names',sep='')
stat.names.af <- paste(stat,'af.names',sep='')
stat.names.aa <- paste(stat,'aa.names',sep='')
feature.hf <- eval(parse(text=stat.names.hf))   #Home-team, for
feature.ha <- eval(parse(text=stat.names.ha))  #Home-team, against
feature.af <- eval(parse(text=stat.names.af)) #Away-team, for
feature.aa <- eval(parse(text=stat.names.aa))   #Away-team, against
dt.predict[row,(feature.hf) := as.list(rev(exp.discount(get.home.stat(dt, stat, hometeam,
date, k))))]
dt.predict[row,(feature.ha) := as.list(rev(exp.discount(get.away.stat(dt, stat, hometeam,
date, k))))]
dt.predict[row,(feature.af) := as.list(rev(exp.discount(get.home.stat(dt, stat, awayteam,
date, k))))]
dt.predict[row,(feature.aa) := as.list(rev(exp.discount(get.away.stat(dt, stat, awayteam,
date, k))))]
}
}
predictions <- predict(xg,xgb.DMatrix(data = as.matrix(dt.predict[,-c('Y','Div')])),xg$best_iteration)
fixtures[,'predictions' := predictions]
fixtures
help(lapply)
lapply(fixtures$predictions,get.accuracy2, pred.vals = pred.vals, Y = test$Y)
lapply(fixtures$predictions,1/get.accuracy2, pred.vals = pred.vals, Y = test$Y)
lapply(fixtures$predictions,get.accuracy2, pred.vals = pred.vals, Y = test$Y)
numbers <- lapply(fixtures$predictions,get.accuracy2, pred.vals = pred.vals, Y = test$Y)
1/numbers
unlist(numbers)
1/unlist(numbers)
impl.odds.adj <- lapply(fixtures$predictions,get.accuracy2, pred.vals = pred.vals, Y = test$Y)
fixtures[,'implied odds_adj' := 1/unlist(impl.odds.adj)]
fixtures
1/0.57
save.image()
F2 <- data.table(read.csv('random/F2.csv'))
F2
tail(F2,5)
fixtures
fixtures[,'actual corners' := rev(c(NA,9,13,9,6,10,9,10,15,9,13,15))]
fixtures
fixtures
8/11
R.Version()
fixtures <- data.table(read.csv('data/future/fixtures_2504.csv'))
fixtures
fixtures$HomeTeam
fixtures$Date
fixtures
fixtures
